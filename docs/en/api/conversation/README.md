# Conversation

## Overview

The `attoboy::Conversation` class represents a **stateful, multi-turn dialogue** with an AI model managed by `attoboy::AI`. Instead of sending isolated prompts, you use a `Conversation` to:

* Maintain a **running history** of messages.
* Ask follow-up questions that automatically include previous context.
* Inspect, modify, or branch the history for advanced workflows.
* Track approximate **token usage** per conversation.

A `Conversation` is created by an `AI` instance with:

```cpp
using namespace attoboy;

AI ai("https://example-base-url", "YOUR_API_KEY", "your-model");
Conversation convo = ai.createConversation();
```

From that point on, all calls to `convo.ask(...)` use the same conversation state, which includes:

* The system prompt configured on the `AI` (if any).
* The alternating sequence of user and assistant messages.
* Token usage statistics for this conversation.

### Multi-turn Conversations and History

In most large language model (LLM) APIs, each request contains not just the latest user message, but a **history** of the entire conversation so far. This allows the model to:

* Refer back to earlier messages.
* Maintain continuity and style.
* Answer follow-up questions naturally.

`Conversation` abstracts this pattern:

* Every call to `ask()` adds a **user message** to the internal history.
* The model’s reply is added as an **assistant message**.
* You can retrieve the full history with `getConversationList()` or replace it with `setConversationList()`.

The history is stored as a `List` of `String` values:

* **Even indices (0, 2, 4, …)** are user messages.
* **Odd indices (1, 3, 5, …)** are assistant messages.

This structure makes it easy to inspect or transform the conversation when needed.

### Tokens and Usage Tracking

Most LLMs are billed and limited by **tokens**, which are small chunks of text (shorter than a word on average). Token usage is an important concept because it affects performance, cost, and sometimes latency.

`Conversation` exposes three counters:

* `getPromptTokensUsed()` – tokens used for prompts (including past conversation turns).
* `getResponseTokensUsed()` – tokens generated by the model as responses.
* `getTotalTokensUsed()` – the sum of prompt and response tokens.

These counters allow you to:

* Monitor approximate usage for analytics or rate limiting.
* Implement simple safety checks such as “warn if the conversation gets too long.”

### Copying and Branching

`Conversation` instances are **handles** to underlying conversation state:

* Copy construction and assignment **share** the same underlying conversation.
* `duplicate()` creates a **branched copy** of the conversation, so that new messages on the duplicate do not affect the original.

This is useful when you want to explore multiple possible “futures” starting from the same conversation history:

* The original `Conversation` continues along one path.
* A duplicate explores different questions or system prompts (through its associated `AI`).

### Creation and Lifetime

You do not construct a `Conversation` directly. The default constructor is private. Instead:

* Call `AI::createConversation()` to obtain a new `Conversation`.
* The `AI` instance manages configuration (model, base URL, key, system prompt, etc.).
* A `Conversation` holds a link back to its `AI` via `getAI()`.

As with other attoboy types, resource management uses RAII:

* The destructor releases the underlying conversation resources when the object goes out of scope.
* Copying and assignment share references cheaply, without copying entire histories.

---

## Reference

Each member of `attoboy::Conversation` is documented below, including signature, synopsis, parameters, return values, detailed behavior, and a short example.

> All examples assume `using namespace attoboy;`.

---

### Constructors, Assignment, and Destruction

#### `Conversation(const Conversation &other)`

**Signature**

```cpp
Conversation(const Conversation &other);
```

**Synopsis**
Creates a copy (shares the underlying conversation).

**Parameters**

* `other` – Existing `Conversation` whose underlying state will be shared.

**Return value**

* *(constructor; not applicable)*

**In Depth**

The copy constructor creates a new `Conversation` object that **refers to the same underlying conversation state** as `other`. This means:

* Both objects see the same message history.
* A call to `ask()` on either object appends to the same conversation.
* Token counters are shared.

This behavior is intentional and efficient: `Conversation` is a lightweight handle that can be passed by value without copying all history.

If you need a separate branch of the conversation (for example, to explore two different follow-up questions independently), use `duplicate()` instead.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation original = ai.createConversation();

Conversation shared(original);  // shares history with 'original'

String reply1 = original.ask("Hello!");
String reply2 = shared.ask("What did I just say?");
```

*This example shows two handles sharing the same conversation state; both `ask()` calls operate on the same history.*

---

#### `~Conversation()`

**Signature**

```cpp
~Conversation();
```

**Synopsis**
Destroys the conversation and frees resources.

**Parameters**

* *(none)*

**Return value**

* *(destructor; not applicable)*

**In Depth**

When a `Conversation` object is destroyed, it releases its reference to the underlying conversation state. If no other `Conversation` objects share that state, the library frees associated resources.

You do not need to call any explicit teardown function. Let the `Conversation` go out of scope, or overwrite the variable, and RAII will ensure clean-up.

**Example**

```cpp
{
  AI ai("https://example-base-url", "KEY", "model");
  Conversation convo = ai.createConversation();
  String reply = convo.ask("Temporary question");
} // convo is destroyed here; its resources are released
```

*This example shows a conversation created and automatically cleaned up when leaving the block.*

---

#### `Conversation &operator=(const Conversation &other)`

**Signature**

```cpp
Conversation &operator=(const Conversation &other);
```

**Synopsis**
Assigns another conversation (shares the underlying conversation).

**Parameters**

* `other` – `Conversation` whose state should be shared after assignment.

**Return value**

* Reference to `*this`, enabling assignment chaining.

**In Depth**

The assignment operator makes the left-hand side `Conversation` share the same underlying conversation state as `other`. Any previously associated conversation state is released.

After assignment:

* Both variables refer to the same history.
* Calls to `ask()` on either variable affect the same conversation.

If you previously had a unique conversation in `*this`, it is no longer referenced and may be freed.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");

Conversation convo1 = ai.createConversation();
Conversation convo2 = ai.createConversation();

convo2 = convo1;  // now both share the same conversation
```

*This example assigns one `Conversation` to another, making them share state.*

---

### Sending Messages

#### `String ask(const String &prompt, int timeout = -1)`

**Signature**

```cpp
String ask(const String &prompt, int timeout = -1);
```

**Synopsis**
Sends a message and returns the response. Check isEmpty() on error.

**Parameters**

* `prompt` – User message to append to the conversation and send to the AI model.
* `timeout` – Optional timeout for the request in milliseconds. A value of `-1` uses the library’s default behavior (typically unlimited or model-specific).

**Return value**

* `String` containing the assistant’s reply.
* If an error occurs (for example, network failure or API error), the returned string is **empty**; you should call `isEmpty()` to check.

**In Depth**

`ask()` is the primary way to interact with an AI through a `Conversation`:

1. The `prompt` is appended to the conversation history as a **user message**.
2. The full history (user and assistant messages) is sent to the AI backend.
3. The AI’s reply is appended as an **assistant message**.
4. The reply is returned as a `String`.

The token usage counters (`getPromptTokensUsed()`, `getResponseTokensUsed()`, `getTotalTokensUsed()`) are updated accordingly.

Error handling is explicit:

* On error, the returned `String` is empty (that is, `reply.isEmpty()` returns `true`).
* The existing conversation history may or may not be updated, depending on the error; you can inspect or reset it with `getConversationList()` and `setConversationList()` if needed.

The `timeout` parameter controls how long the call may block waiting for a response. Use it if you need to prevent long-running calls from blocking your application.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

String reply = convo.ask("Hello, who are you?");

if (!reply.isEmpty()) {
  // Use the reply, and optionally ask follow-up questions.
  String followUp = convo.ask("Can you summarize your abilities?");
}
```

*This example starts a conversation, sends a question, checks for errors by testing `isEmpty()`, and then asks a follow-up using the same conversation context.*

---

### Conversation History

#### `List getConversationList() const`

**Signature**

```cpp
List getConversationList() const;
```

**Synopsis**
Returns the conversation history (even=user, odd=assistant).

**Parameters**

* *(none)*

**Return value**

* `List` of `String` messages representing the conversation history.

  * **Even indices (0, 2, 4, …)** are user messages.
  * **Odd indices (1, 3, 5, …)** are assistant messages.

**In Depth**

`getConversationList()` exposes the internal history in a simple, generic format:

* The list is a flat sequence of messages.
* The index parity (even/odd) tells you who spoke.
* Messages are stored as `String` values.

Typical uses:

* Logging or debugging the conversation.
* Implementing custom conversation views or saving history to disk.
* Copying the history to seed a new conversation (via `setConversationList()`).

Because the returned `List` is a separate object, modifying it does **not** automatically change the underlying conversation. To apply changes, call `setConversationList()` with the modified list.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

convo.ask("Hello!");
convo.ask("What can you do?");

List history = convo.getConversationList();
int count = history.length();

// Example: get the last user message, if any.
if (count >= 1 && (count - 1) % 2 == 0) {
  String lastUser = history.at<String>(count - 1);
}
```

*This example retrieves the history and demonstrates how to locate the last user message using index parity.*

---

#### `bool setConversationList(const List &list)`

**Signature**

```cpp
bool setConversationList(const List &list);
```

**Synopsis**
Replaces the conversation history. Returns true on success.

**Parameters**

* `list` – `List` of `String` messages to use as the new conversation history. The convention is:

  * Even indices are user messages.
  * Odd indices are assistant messages.

**Return value**

* `true` if the history was successfully replaced.
* `false` if the list could not be applied (for example, if it is malformed or too large for the underlying implementation).

**In Depth**

`setConversationList()` lets you **override** the internal conversation history. Common scenarios:

* Restoring a saved conversation from disk.
* Starting from a pre-defined “script” of messages.
* Editing out sensitive messages before continuing.

When you call this function:

* The existing history is replaced entirely.
* Subsequent calls to `ask()` continue from the new history.
* Token counters may reset or recompute depending on the implementation; do not rely on them being preserved unless you manage them externally.

You are responsible for constructing `list` correctly (using even/odd indices for user/assistant). The function does not reinterpret the messages; it assumes you follow the documented convention.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

// Build a scripted conversation history.
List scripted;
scripted.append("User: Please always answer very briefly.");
scripted.append("Assistant: Understood. I will answer briefly.");
scripted.append("User: What is 2+2?");

// Apply it to the conversation.
bool ok = convo.setConversationList(scripted);

if (ok) {
  String reply = convo.ask("And what is 3+3?");
}
```

*This example seeds the conversation with a pre-defined history and then continues with a new question.*

---

### Branching and Associated AI

#### `Conversation duplicate() const`

**Signature**

```cpp
Conversation duplicate() const;
```

**Synopsis**
Creates a copy of this conversation for branching.

**Parameters**

* *(none)*

**Return value**

* A new `Conversation` that starts with the same history as the original, but evolves independently afterwards.

**In Depth**

Unlike the copy constructor and assignment (which share the underlying conversation), `duplicate()` creates a **branched** conversation:

* The new `Conversation` begins with the same message history.
* Further calls to `ask()` on the original and duplicate modify **separate** histories.
* Token tracking for each branch is independent.

Use `duplicate()` when you want to explore alternative follow-ups from the same starting context.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation base = ai.createConversation();

base.ask("Explain recursion in simple terms.");

Conversation branch1 = base.duplicate();
Conversation branch2 = base.duplicate();

String reply1 = branch1.ask("Give me a very short example.");
String reply2 = branch2.ask("Give me a detailed, step-by-step example.");
```

*This example creates two branches from a shared initial conversation and asks different follow-up questions on each branch.*

---

#### `AI getAI() const`

**Signature**

```cpp
AI getAI() const;
```

**Synopsis**
Returns the AI instance managing this conversation.

**Parameters**

* *(none)*

**Return value**

* An `AI` object that shares the underlying configuration with the `Conversation`’s original `AI`.

**In Depth**

`getAI()` gives you access to the `AI` instance associated with the `Conversation`. This is useful when you need to:

* Inspect configuration (model name, base URL, system prompt).
* Adjust settings (for example, `setMaxTokens()` or `setJsonMode()`).
* Create additional conversations with the same AI configuration.

The returned `AI` shares underlying configuration with the original `AI` that created the conversation. Changes made through this `AI` instance are reflected across conversations that share it.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

// Retrieve the AI from the conversation.
AI associated = convo.getAI();

// Change configuration (for example, set JSON mode).
associated.setJsonMode(true);

String reply = convo.ask("Respond with JSON describing a cat.");
```

*This example retrieves the managing `AI` from a conversation and updates its configuration before asking a question.*

---

### Token Usage Tracking

#### `int getPromptTokensUsed() const`

**Signature**

```cpp
int getPromptTokensUsed() const;
```

**Synopsis**
Returns the number of prompt tokens used.

**Parameters**

* *(none)*

**Return value**

* Integer count of tokens used in prompts for this conversation (as tracked by the implementation).

**In Depth**

This counter represents the cumulative number of tokens sent to the model as **input**:

* Includes the system prompt (if any).
* Includes all user and assistant messages that have been part of requests.

Use this to:

* Monitor usage per conversation.
* Implement simple limits (for example, finish or reset a conversation after a token threshold).

The exact counting method depends on the underlying model and API; treat it as an approximate but useful metric rather than a strict billing value.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

convo.ask("Hello!");
convo.ask("Tell me a story.");

int promptTokens = convo.getPromptTokensUsed();
```

*This example retrieves the number of prompt tokens used after a few turns.*

---

#### `int getResponseTokensUsed() const`

**Signature**

```cpp
int getResponseTokensUsed() const;
```

**Synopsis**
Returns the number of response tokens used.

**Parameters**

* *(none)*

**Return value**

* Integer count of tokens generated by the model as **responses** in this conversation.

**In Depth**

This counter tracks how many tokens the model has produced as output across all calls to `ask()` on this conversation.

It can help you:

* Estimate how “verbose” the assistant has been.
* Implement policies based on total generated content.

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

convo.ask("Explain tokens.");
convo.ask("Summarize your explanation in one sentence.");

int responseTokens = convo.getResponseTokensUsed();
```

*This example checks how many output tokens have been generated so far.*

---

#### `int getTotalTokensUsed() const`

**Signature**

```cpp
int getTotalTokensUsed() const;
```

**Synopsis**
Returns the total tokens used.

**Parameters**

* *(none)*

**Return value**

* Integer equal to `getPromptTokensUsed() + getResponseTokensUsed()`.

**In Depth**

This is the **overall token usage** for the conversation, combining both prompt and response tokens. It is the most convenient single number for:

* Budgeting usage per conversation.
* Deciding when to truncate or reset long histories.
* Simple monitoring dashboards.

The relationship is:

```cpp
getTotalTokensUsed() == getPromptTokensUsed() + getResponseTokensUsed();
```

**Example**

```cpp
AI ai("https://example-base-url", "KEY", "model");
Conversation convo = ai.createConversation();

convo.ask("Hello!");
convo.ask("What can you do?");
convo.ask("Give me three bullet points.");

int total = convo.getTotalTokensUsed();

if (total > 4000) {
  // Consider truncating or resetting the conversation.
}
```

*This example uses the total token count to decide when a conversation may have grown too large.*
