# AI

## Overview

The `attoboy::AI` class is the main entry point for working with remote AI models (large language models and embeddings) in the attoboy library. It acts as an **OpenAI-compatible API client** for:

* **Chat completions** – send a prompt and receive a generated response as text.
* **Embeddings** – convert text into numeric vectors for semantic similarity.
* **Conversations** – manage multi-turn chat sessions with history tracking.

You can think of `AI` as a **configured handle** to a particular AI service: it stores the base URL of the API endpoint, your API key, and the default model name, and it provides methods to:

* Adjust how requests are made (model, system prompt, maximum tokens, JSON mode).
* Observe usage and behavior (token counts and finish reason).
* Perform actual operations (`ask`, `createEmbedding`, `createConversation`).

### Remote AI Services, Base URLs, and API Keys

The `AI` class does not run AI models locally. Instead, it talks to a **remote HTTP API** that follows an OpenAI-style interface. To construct an `AI` instance, you provide:

* `baseUrl` – The root URL of the AI service (for example, `"https://api.openai.com/v1"`).
* `apiKey` – Your secret key used to authenticate requests.
* `model` – Name of the model to call (for example, `"gpt-5-mini"` or vendor-specific variants).

These values are stored inside the `AI` object and reused for each subsequent request. You can change the model later with `setModel`.

> Treat your API key like a password: do not commit it to source control or log it.

### Prompts, System Prompts, and Responses

When you use `AI::ask`, you send a **prompt**:

* The `prompt` argument is a `String` that contains the user’s question, command, or input text.
* The AI service returns a response as a `String` (for example, an answer, explanation, or completion).

In addition, you can set a **system prompt** via `setSystemPrompt`. A system prompt:

* Describes the overall behavior or “role” of the AI (for example, “You are a concise assistant that answers in JSON.”).
* Applies to all subsequent calls from that `AI` instance and any conversations created from it.
* Can be cleared by passing an empty String to `setSystemPrompt`.

In practice, this lets you separate **global behavior** (system prompt) from **per-request content** (prompt text passed to `ask` or `Conversation::ask`).

### Tokens, Limits, and Usage Tracking

AI services often measure usage in **tokens**, which are small pieces of text (shorter than words). Each request consumes:

* **Prompt tokens** – tokens from your inputs (system prompt, messages, etc.).
* **Response tokens** – tokens generated by the model in its reply.

The `AI` class tracks usage cumulatively across all calls on the same instance:

* `getPromptTokensUsed()` – total prompt tokens.
* `getResponseTokensUsed()` – total response tokens.
* `getTotalTokensUsed()` – sum of the two.

This tracking is useful for:

* Monitoring costs when usage is billed by tokens.
* Debugging performance issues when responses are unexpectedly long.

You can reset the counters at any time with `resetTokenTracking()`.

You can also control how many tokens a response is allowed to use via `setMaxTokens(int max)`. A typical pattern is:

* Set a global limit (for example, `setMaxTokens(256)`).
* Use `getFinishReason()` to see whether a response was cut off due to length (finish reason `"length"`) or ended naturally (finish reason `"stop"`).

### JSON Mode

Many AI workflows involve generating structured output. `AI::setJsonMode(bool)` enables a **JSON response mode**:

* When enabled, the prompt and system prompt should instruct the model to respond with valid JSON.
* The AI client configures the request to favor JSON shapes when supported by the remote API.
* You then parse the response body (as `String`) as JSON using your own logic or other attoboy types.

This mode is particularly useful when building command-line tools or services that consume AI responses programmatically.

### Embeddings and Semantic Similarity

The `AI::createEmbedding` method converts text into an `Embedding` object:

* An `Embedding` represents a high-dimensional numeric vector (`float` values) that captures semantic meaning.
* Vectors for similar texts are close to each other under cosine similarity.

The usual workflow is:

1. Call `createEmbedding` for one or more pieces of text.
2. Use `Embedding::compare` to compute similarity scores between them.
3. Use these scores for search, clustering, or ranking.

If `createEmbedding` fails, it returns an `Embedding` with `getDimensions() == 0`.

### Conversations vs. Single-Shot Calls

There are two ways to interact with the model:

1. **Single-shot** (`AI::ask`)

   * You pass a single prompt and receive a single response.
   * Useful for simple utilities and one-off questions.

2. **Multi-turn** (`AI::createConversation`)

   * You create a `Conversation` object tied to the `AI` instance.
   * Each call to `Conversation::ask` adds user and assistant messages to history.
   * The model sees the full context of the conversation.

Both approaches share the same underlying configuration (base URL, API key, model, system prompt, token settings). Changes to an `AI` instance apply to future operations and new conversations created from it.

---

## Reference

Each entry below covers one public constructor, method, or operator of `attoboy::AI`. For each, you will find:

* **Signature** – the exact declaration from the header.
* **Synopsis** – the original one-line Doxygen comment.
* **Parameters** and **Return value** descriptions.
* **In Depth** – behavior details, caveats, and a usage example.

> All code examples assume `using namespace attoboy;`.

---

#### `AI(const String &baseUrl, const String &apiKey, const String &model)`

**Signature**

```cpp
AI(const String &baseUrl, const String &apiKey, const String &model);
```

**Synopsis**
Creates an AI client with base URL, API key, and model name.

**Parameters**

* `baseUrl` – Base URL of the AI service (for example, `"https://api.openai.com/v1"`).
* `apiKey` – Secret API key used for authentication.
* `model` – Default model name to use for chat and embedding calls.

**Return value**

* *(constructor; not applicable)*

**In Depth**

This constructor initializes an `AI` client with the three core pieces of configuration it needs to communicate with a remote service:

* **Base URL** – All requests are sent relative to this root.
* **API key** – Typically passed as an HTTP header (for example, `Authorization: Bearer <key>`).
* **Model name** – Used to select which model the service should run (for example, chat vs. embedding variants).

Once constructed, the `AI` instance can be reused across many requests. You can later adjust the model with `setModel`, or modify other behavior (system prompt, JSON mode, max tokens) without reconstructing the client.

If the configuration is invalid (for example, wrong URL or key), requests such as `ask` and `createEmbedding` will fail; you should detect this by checking the returned `String` or `Embedding` objects.

**Example**

```cpp
using namespace attoboy;

String baseUrl("https://api.openai.com/v1");
String key("sk-123456");
String model("gpt-5-mini");

AI ai(baseUrl, key, model);
```

*This example constructs an `AI` client configured to talk to a hypothetical remote service using a specific model.*

---

#### `AI(const AI &other)`

**Signature**

```cpp
AI(const AI &other);
```

**Synopsis**
Creates a copy (shares the underlying configuration).

**Parameters**

* `other` – Existing `AI` instance to copy.

**Return value**

* *(constructor; not applicable)*

**In Depth**

The copy constructor creates a new `AI` object that shares the same underlying configuration and internal implementation as `other`. This is a **shallow copy**:

* Base URL, API key, model, system prompt, and token tracking configuration are shared logically.
* Using either instance to send requests or adjust configuration affects the underlying client state.

This is convenient when you want to pass `AI` by value to helper code without worrying about lifetime or expensive copies.

**Example**

```cpp
using namespace attoboy;

AI original(String("https://api.openai.com/v1"),
            String("sk-123456"),
            String("gpt-5-mini"));

AI copy(original);
```

*This example creates a second `AI` handle that shares the same configuration as the original.*

---

#### `~AI()`

**Signature**

```cpp
~AI();
```

**Synopsis**
Destroys the AI client and frees resources.

**Parameters**

* *(none)*

**Return value**

* *(destructor; not applicable)*

**In Depth**

When an `AI` object is destroyed (for example, when it goes out of scope), the destructor releases any resources associated with the client:

* Internal handles and buffers.
* Network and configuration state held in `AIImpl`.

Because `AI` uses RAII, you do not need to manually close network connections; the underlying implementation takes care of cleanup.

If there are other `AI` instances that share the same implementation (via copy construction or assignment), the implementation remains alive until the last handle is destroyed.

**Example**

```cpp
using namespace attoboy;

{
  AI ai(String("https://api.openai.com/v1"),
        String("sk-123456"),
        String("gpt-5-mini"));
  // Use ai here
} // ai is destroyed; its resources are cleaned up
```

*This example shows an `AI` instance with automatic lifetime inside a block.*

---

#### `AI &operator=(const AI &other)`

**Signature**

```cpp
AI &operator=(const AI &other);
```

**Synopsis**
Assigns another AI client (shares the underlying configuration).

**Parameters**

* `other` – Existing `AI` to assign from.

**Return value**

* Reference to `*this`, enabling assignment chaining.

**In Depth**

The assignment operator makes the left-hand side `AI` share the same internal implementation and configuration as `other`. Any previous association of `*this` is released, and any resources that were uniquely owned may be freed.

After assignment:

* Both `AI` objects refer to the same base URL, API key, model, system prompt, and token tracking state.
* Sending requests through either handle uses the shared configuration.

Self-assignment is safe; the implementation should detect it and avoid unnecessary work.

**Example**

```cpp
using namespace attoboy;

AI ai1(String("https://api.openai.com/v1"),
       String("sk-1"),
       String("gpt-5-mini"));

AI ai2(String("https://api.other.com/v1"),
       String("sk-2"),
       String("gpt-5-mini"));

ai2 = ai1;
// ai2 now shares ai1's configuration
```

*This example reassigns an `AI` instance to share the configuration of another.*

---

#### `AI &setModel(const String &model)`

**Signature**

```cpp
AI &setModel(const String &model);
```

**Synopsis**
Sets the model name. Returns this AI for chaining.

**Parameters**

* `model` – New model name to use for subsequent requests.

**Return value**

* Reference to `*this` (the same `AI` object), enabling fluent chaining.

**In Depth**

`setModel` updates the default model for all future operations performed with this `AI` instance and any new conversations created from it. It does not affect operations that have already been sent.

This is useful when:

* You want to switch between lightweight and heavyweight models (for example, `"small-quick"` vs `"large-accurate"`).
* You want to use one model for chat and another for embeddings via separate `AI` instances.

Because the method returns `*this`, you can chain calls:

```cpp
ai.setModel("gpt-5-mini").setMaxTokens(256);
```

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setModel("gpt-4.1-large");
```

*This example switches the AI client to a different model for future calls.*

---

#### `AI &setSystemPrompt(const String &prompt)`

**Signature**

```cpp
AI &setSystemPrompt(const String &prompt);
```

**Synopsis**
Sets the system prompt (empty string to clear). Returns this AI for chaining.

**Parameters**

* `prompt` – `String` holding the system prompt, or an empty string to clear it.

**Return value**

* Reference to `*this`, enabling fluent chaining.

**In Depth**

The system prompt provides **global instructions** that influence how the AI behaves across all requests for this `AI` instance:

* It typically describes the assistant's role, style, and constraints.
* It is combined with each user prompt during calls to `ask` and `Conversation::ask`.

To set a system prompt, pass a `String` containing the instructions.

To clear the system prompt, pass an empty string. After the call, the client returns to having no system prompt.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String sys(
  "You are a helpful assistant that answers concisely.\n"
  "Use simple language and avoid technical jargon."
);

ai.setSystemPrompt(sys);
```

*This example configures a system prompt that shapes how the AI responds to all future requests.*

---

#### `AI &setMaxTokens(int max = -1)`

**Signature**

```cpp
AI &setMaxTokens(int max = -1);
```

**Synopsis**
Sets max response tokens (-1 for model default). Returns this AI for chaining.

**Parameters**

* `max` – Maximum number of response tokens the model is allowed to generate.

  * `-1` uses the model’s default limit (no explicit override).

**Return value**

* Reference to `*this`, enabling fluent chaining.

**In Depth**

`setMaxTokens` controls the **maximum length** of generated responses:

* Smaller values reduce cost and latency but can cut off longer answers.
* Larger values allow more detailed responses but may be slower and more expensive.

When a response is cut short due to this limit, `getFinishReason()` typically returns `"length"`. When the model stops naturally, it returns `"stop"` or another reason.

This setting applies to:

* `ask`
* `Conversation::ask`
* Possibly embedding requests (depending on the remote API), but primarily for chat completions.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setMaxTokens(128);  // limit responses to 128 tokens
```

*This example applies a token limit to keep responses short and predictable.*

---

#### `AI &setJsonMode(bool isJsonMode = false)`

**Signature**

```cpp
AI &setJsonMode(bool isJsonMode = false);
```

**Synopsis**
Enables/disables JSON response mode. Returns this AI for chaining.

**Parameters**

* `isJsonMode` – `true` to enable JSON response mode; `false` to disable.

**Return value**

* Reference to `*this`, enabling fluent chaining.

**In Depth**

When JSON mode is enabled:

* The `AI` client configures the request so that the model is expected to return a JSON structure.
* Your prompts and system prompts should instruct the model clearly to respond with valid JSON (for example, a single JSON object or array).

This mode is particularly useful for:

* Tools that parse responses programmatically using attoboy `Map`/`List` after parsing JSON.
* Workflows that depend on structured output (for example, classification labels, configuration objects).

JSON mode does not automatically parse the response; `ask` still returns a `String`. You are responsible for parsing that string into structured data as needed.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setJsonMode(true);
```

*This example enables JSON mode so that subsequent responses are expected to be JSON-formatted.*

---

#### `String getModel() const`

**Signature**

```cpp
String getModel() const;
```

**Synopsis**
Returns the current model name.

**Parameters**

* *(none)*

**Return value**

* `String` containing the name of the model currently configured for this `AI`.

**In Depth**

`getModel()` allows you to inspect which model the client is currently using. This is useful for:

* Debugging (logging configuration before a request).
* Building tools that report or display model selection.

The returned `String` is an immutable copy of the internal model name.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String currentModel = ai.getModel();  // "gpt-5-mini"
```

*This example retrieves the current model name for logging or display.*

---

#### `String getSystemPrompt() const`

**Signature**

```cpp
String getSystemPrompt() const;
```

**Synopsis**
Returns the system prompt. Check isEmpty() to see if set.

**Parameters**

* *(none)*

**Return value**

* `String` containing the current system prompt, or an empty string if none is set.

**In Depth**

`getSystemPrompt()` returns whatever was last configured via `setSystemPrompt`, or an empty string if the system prompt has never been set or has been cleared.

Because an empty string is also a valid prompt, use `isEmpty()` on the returned value to distinguish “no prompt set” from “explicitly set to empty”.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String sys = ai.getSystemPrompt();
if (sys.isEmpty()) {
  // No system prompt configured
}
```

*This example checks whether a system prompt has been configured.*

---

#### `String getBaseUrl() const`

**Signature**

```cpp
String getBaseUrl() const;
```

**Synopsis**
Returns the base URL.

**Parameters**

* *(none)*

**Return value**

* `String` containing the base URL for this `AI` client.

**In Depth**

`getBaseUrl()` exposes the base endpoint that this `AI` instance is configured to use. This is useful for:

* Debugging multi-environment setups (for example, staging vs. production).
* Logging configuration details when running diagnostics.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String url = ai.getBaseUrl();  // "https://api.openai.com/v1"
```

*This example queries the base URL to confirm which environment the client is talking to.*

---

#### `String getAPIKey() const`

**Signature**

```cpp
String getAPIKey() const;
```

**Synopsis**
Returns the API key.

**Parameters**

* *(none)*

**Return value**

* `String` containing the API key.

**In Depth**

`getAPIKey()` returns the API key stored in this `AI` instance. This is primarily useful for:

* Confirming that the client has been configured with **some** key.
* Passing the key into other APIs that might need it.

Because API keys are sensitive, you should avoid logging them directly. If you must log something, consider masking or truncating the key before logging.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String key = ai.getAPIKey();
// Avoid printing key directly in logs.
```

*This example retrieves the API key, with a reminder to treat it as sensitive.*

---

#### `int getPromptTokensUsed() const`

**Signature**

```cpp
int getPromptTokensUsed() const;
```

**Synopsis**
Returns cumulative prompt tokens used.

**Parameters**

* *(none)*

**Return value**

* Total number of prompt tokens consumed by this `AI` instance since creation or the last call to `resetTokenTracking()`.

**In Depth**

Each time you call `ask` or use a `Conversation` created from this `AI`, prompt tokens are counted and added to this total. Prompt tokens typically include:

* System prompt tokens.
* Conversation or message history.
* The new input prompt.

Use this value to monitor how much input you have sent to the model over time.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Hello!", -1);
int promptTokens = ai.getPromptTokensUsed();
```

*This example retrieves the accumulated prompt token count after making a request.*

---

#### `int getResponseTokensUsed() const`

**Signature**

```cpp
int getResponseTokensUsed() const;
```

**Synopsis**
Returns cumulative response tokens used.

**Parameters**

* *(none)*

**Return value**

* Total number of response tokens generated by the model for this `AI` instance since creation or the last `resetTokenTracking()`.

**In Depth**

Response tokens measure how much **output** the model has generated in response to your requests. This includes:

* One-shot responses from `ask`.
* Responses from `Conversation::ask`.

This count is useful for monitoring and limiting overall output volume.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Explain RAII briefly.", -1);
int responseTokens = ai.getResponseTokensUsed();
```

*This example queries the total number of tokens produced by the model so far.*

---

#### `int getTotalTokensUsed() const`

**Signature**

```cpp
int getTotalTokensUsed() const;
```

**Synopsis**
Returns cumulative total tokens used.

**Parameters**

* *(none)*

**Return value**

* Sum of prompt and response tokens consumed by this `AI` instance.

**In Depth**

`getTotalTokensUsed()` is a convenient shortcut for:

```cpp
getPromptTokensUsed() + getResponseTokensUsed()
```

Use this when you are interested in total usage, such as for approximate cost calculations.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Give me a short poem.", -1);

int totalTokens = ai.getTotalTokensUsed();
```

*This example obtains the combined prompt and response token usage.*

---

#### `void resetTokenTracking()`

**Signature**

```cpp
void resetTokenTracking();
```

**Synopsis**
Resets all token counters to zero.

**Parameters**

* *(none)*

**Return value**

* *(void; not applicable)*

**In Depth**

Calling `resetTokenTracking()` sets:

* `getPromptTokensUsed()` → 0
* `getResponseTokensUsed()` → 0
* `getTotalTokensUsed()` → 0

This is useful when you want to:

* Start a new accounting period (for example, per command execution or per user session).
* Discard previous counts before measuring the usage of a single operation.

The reset affects only future calls; previous usage is not recoverable once reset.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("First request.", -1);
ai.resetTokenTracking();
ai.ask("Second request.", -1);

int tokensForSecond = ai.getTotalTokensUsed();
```

*This example resets token tracking after the first request and measures usage only for the second.*

---

#### `String getFinishReason() const`

**Signature**

```cpp
String getFinishReason() const;
```

**Synopsis**
Returns the finish reason from the last call (e.g., "stop", "length").

**Parameters**

* *(none)*

**Return value**

* `String` describing why the last model call stopped generating output.

**In Depth**

`getFinishReason()` provides insight into how the last `ask` or `Conversation::ask` ended. Common values include:

* `"stop"` – The model ended naturally (for example, hit an end-of-sequence token or satisfied constraints).
* `"length"` – The model was stopped because it reached the maximum token limit (`setMaxTokens` or service limit).
* Other service-specific values (for example, `"content_filter"`, `"tool_calls"`, etc.).

If no request has been made yet, the finish reason may be an empty string or some default value, depending on the implementation.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setMaxTokens(16);
String answer = ai.ask("Explain RAII in detail.", -1);

String reason = ai.getFinishReason();
// e.g., "length" if the explanation was cut short
```

*This example checks whether the response was truncated due to the token limit.*

---

#### `String ask(const String &prompt, int timeout = -1)`

**Signature**

```cpp
String ask(const String &prompt, int timeout = -1);
```

**Synopsis**
Sends a single prompt and returns the response. Check isEmpty() on error.

**Parameters**

* `prompt` – User input or instruction text for the model.
* `timeout` – Timeout in milliseconds.

  * `-1` means “use the model or client default” (often treated as infinite or a configured default).

**Return value**

* `String` containing the model’s response text if successful.
* An empty `String` if an error occurs (for example, network failure, invalid configuration, or service error).

**In Depth**

`ask` is the simplest way to interact with the AI:

* It constructs a single-shot request using the current configuration:

  * Base URL, API key, model.
  * System prompt (if any).
  * Max tokens and JSON mode.
* It waits for the response (or until `timeout` is reached).
* It returns the model’s reply as a `String`.

Typical error detection patterns:

* If `ask` returns an empty string, you should treat it as a failure and handle it appropriately (for example, by logging or retrying).
* Use `getFinishReason()` to see how the request ended.
* Use token tracking methods to measure usage.

**Timeout behavior:**

* A positive `timeout` value limits how long the client waits for a response.
* `-1` leaves timeout behavior to the implementation and remote service.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String response = ai.ask("Give me a one-line definition of RAII.", 5000);

if (!response.isEmpty()) {
  // Use the response, e.g. print or log it
}
```

*This example sends a prompt with a 5-second timeout and checks for an empty response to detect errors.*

---

#### `Embedding createEmbedding(const String &str, int dimensions = -1, int timeout = -1)`

**Signature**

```cpp
Embedding createEmbedding(const String &str, int dimensions = -1,
                          int timeout = -1);
```

**Synopsis**
Creates an embedding vector. Check getDimensions() == 0 on error.

**Parameters**

* `str` – Text to embed (the input whose semantic meaning you want to capture).
* `dimensions` – Desired number of dimensions for the embedding:

  * `-1` lets the model choose its default dimensionality.
  * Positive values request a specific size, if supported by the service.
* `timeout` – Timeout in milliseconds for the request (`-1` for default behavior).

**Return value**

* An `Embedding` object representing the vector.
* On error, an `Embedding` where `getDimensions() == 0`.

**In Depth**

Embeddings convert text into numeric vectors that capture semantic similarity:

* Similar texts yield embeddings with higher cosine similarity (close to `1.0`).
* Dissimilar texts produce embeddings with lower or negative similarity.

After calling `createEmbedding`:

* Check `embedding.getDimensions()`:

  * `> 0` – success.
  * `0` – error (invalid configuration, network issue, or service error).
* Use `Embedding::compare` to compute similarity between two embeddings.

The `dimensions` parameter allows you to control vector size when the service supports multiple sizes. Smaller vectors are faster and take less memory; larger vectors may be more accurate.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("text-embedding-model"));

Embedding e1 = ai.createEmbedding("Hello world", -1, 5000);
Embedding e2 = ai.createEmbedding("Hi there", -1, 5000);

if (e1.getDimensions() > 0 && e2.getDimensions() > 0) {
  float similarity = e1.compare(e2);
  // similarity close to 1.0 means texts are semantically similar
}
```

*This example creates embeddings for two short phrases and compares them using cosine similarity.*

---

#### `Conversation createConversation()`

**Signature**

```cpp
Conversation createConversation();
```

**Synopsis**
Creates a new multi-turn conversation.

**Parameters**

* *(none)*

**Return value**

* A `Conversation` object that is linked to this `AI` instance.

**In Depth**

`createConversation` constructs a `Conversation` that:

* Uses the same base URL, API key, model, system prompt, max tokens, and JSON mode as the `AI` instance that created it.
* Maintains its own message history:

  * Each call to `Conversation::ask` adds user and assistant messages to the conversation.
  * The model sees the entire history when generating new replies.

Use `Conversation` when you:

* Need stateful interactions where each answer depends on previous questions.
* Want to maintain a chat-like interface inside your application.

You can create multiple conversations from a single `AI` instance; each one has its own independent history.

**Example**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

Conversation conv = ai.createConversation();

String reply1 = conv.ask("Hello, who are you?", -1);
String reply2 = conv.ask("Can you remind me what I just asked?", -1);
```

*This example creates a conversation and performs two turns of dialogue, with the second question depending on the first.*

