# AI

## Обзор

Класс `attoboy::AI` является основной точкой входа для работы с удалёнными AI-моделями (большие языковые модели и эмбеддинги) в библиотеке attoboy. Он выступает в роли **клиента API, совместимого с OpenAI**, для:

* **Чат-ответов (chat completions)** – отправка промпта и получение сгенерированного ответа в виде текста.
* **Эмбеддингов (embeddings)** – преобразование текста в числовые векторы для семантического сравнения.
* **Разговоров (conversations)** – управление многоходовыми чат-сессиями с учётом истории.

Можно считать `AI` как **настроенный дескриптор** конкретного AI-сервиса: он хранит базовый URL конечной точки API, ваш API-ключ и имя модели по умолчанию, а также предоставляет методы для:

* Настройки того, как выполняются запросы (модель, системный промпт, максимальное число токенов, JSON-режим).
* Наблюдения за использованием и поведением (подсчёт токенов и причина завершения).
* Выполнения реальных операций (`ask`, `createEmbedding`, `createConversation`).

### Удалённые AI-сервисы, базовые URL и API-ключи

Класс `AI` не выполняет модели локально. Вместо этого он общается с **удалённым HTTP API**, который следует интерфейсу в стиле OpenAI. Для создания экземпляра `AI` вы указываете:

* `baseUrl` – корневой URL сервиса AI (например, `"https://api.openai.com/v1"`).
* `apiKey` – ваш секретный ключ для аутентификации запросов.
* `model` – имя модели, которую следует вызывать (например, `"gpt-5-mini"` или варианты от поставщика).

Эти значения сохраняются внутри объекта `AI` и используются при каждом последующем запросе. Модель можно изменить позже с помощью `setModel`.

> Обращайтесь с вашим API-ключом как с паролем: не коммитьте его в систему контроля версий и не логируйте.

### Промпты, системные промпты и ответы

Когда вы используете `AI::ask`, вы отправляете **промпт**:

* Аргумент `prompt` — это `String`, содержащий вопрос пользователя, команду или входной текст.
* Сервис AI возвращает ответ в виде `String` (например, ответ, объяснение или дополнение).

Кроме того, вы можете задать **системный промпт** через `setSystemPrompt`. Системный промпт:

* Описывает общее поведение или «роль» AI (например, «You are a concise assistant that answers in JSON.»).
* Применяется ко всем последующим вызовам из этого экземпляра `AI` и ко всем разговором, созданным из него.
* Может быть очищен, если передать пустую строку в `setSystemPrompt`.

На практике это позволяет разделять **глобальное поведение** (системный промпт) и **содержимое конкретного запроса** (текст промпта, передаваемый в `ask` или `Conversation::ask`).

### Токены, лимиты и отслеживание использования

Сервисы AI часто измеряют использование в единицах **токенов**, которые являются небольшими фрагментами текста (короче слов). Каждый запрос потребляет:

* **Промпт-токены** – токены из ваших входных данных (системный промпт, сообщения и т. п.).
* **Токены ответа** – токены, сгенерированные моделью в ответе.

Класс `AI` ведёт кумулятивный учёт использования по всем вызовам на одном экземпляре:

* `getPromptTokensUsed()` – всего промпт-токенов.
* `getResponseTokensUsed()` – всего токенов ответа.
* `getTotalTokensUsed()` – сумма двух значений.

Этот учёт полезен для:

* Мониторинга затрат, если оплата идёт по токенам.
* Отладки проблем с производительностью, когда ответы оказываются неожиданно длинными.

Вы можете сбросить счётчики в любой момент с помощью `resetTokenTracking()`.

Также вы можете контролировать, сколько токенов может использовать ответ, с помощью `setMaxTokens(int max)`. Типичная схема:

* Установить глобальный лимит (например, `setMaxTokens(256)`).
* Использовать `getFinishReason()` чтобы увидеть, был ли ответ усечён из-за длины (finish reason `"length"`) или завершился естественно (finish reason `"stop"`).

### JSON-режим

Многие рабочие процессы с AI предполагают генерацию структурированного вывода. `AI::setJsonMode(bool)` включает **JSON-режим ответа**:

* При включённом режиме промпт и системный промпт должны инструктировать модель отвечать валидным JSON.
* Клиент AI настраивает запрос так, чтобы удалённый API при возможности отдавал предпочтение JSON-формату.
* Затем вы парсите тело ответа (как `String`) в JSON с помощью собственной логики или других типов attoboy.

Этот режим особенно полезен при создании командных утилит или сервисов, которые программно обрабатывают ответы AI.

### Эмбеддинги и семантическое сходство

Метод `AI::createEmbedding` преобразует текст в объект `Embedding`:

* `Embedding` представляет собой вектор высоких размерностей (значения `float`), отражающий семантический смысл.
* Векторы для похожих текстов расположены близко друг к другу по косинусному сходству.

Обычный рабочий поток:

1. Вызвать `createEmbedding` для одного или нескольких текстов.
2. Использовать `Embedding::compare` для вычисления оценок сходства между ними.
3. Применять эти оценки для поиска, кластеризации или ранжирования.

Если `createEmbedding` завершится неудачей, он вернёт `Embedding` с `getDimensions() == 0`.

### Разговоры vs одноразовые вызовы

Существует два способа взаимодействия с моделью:

1. **Одноразовый вызов** (`AI::ask`)

   * Вы передаёте один промпт и получаете один ответ.
   * Полезно для простых утилит и одноразовых вопросов.

2. **Многоходовой (multi-turn)** (`AI::createConversation`)

   * Вы создаёте объект `Conversation`, привязанный к экземпляру `AI`.
   * Каждый вызов `Conversation::ask` добавляет сообщения пользователя и ассистента в историю.
   * Модель видит полный контекст разговора.

Оба подхода используют одну и ту же конфигурацию (base URL, API key, model, system prompt, настройки токенов). Изменения в экземпляре `AI` применяются к будущим операциям и новым разговором, созданным из него.

---

## Справочник

Каждый пункт ниже описывает один публичный конструктор, метод или оператор `attoboy::AI`. Для каждого приведено:

* **Сигнатура** – точное объявление из заголовка.
* **Краткое описание** – оригинальный однострочный комментарий Doxygen.
* **Параметры** и **Описание возвращаемого значения**.
* **Подробно** – поведение, оговорки и пример использования.

> Все примеры кода предполагают `using namespace attoboy;`.

---

#### `AI(const String &baseUrl, const String &apiKey, const String &model)`

**Сигнатура**

```cpp
AI(const String &baseUrl, const String &apiKey, const String &model);
```

**Краткое описание**
Создаёт клиент AI с базовым URL, API-ключом и именем модели.

**Параметры**

* `baseUrl` – Базовый URL сервиса AI (например, `"https://api.openai.com/v1"`).
* `apiKey` – Секретный API-ключ, используемый для аутентификации.
* `model` – Имя модели по умолчанию для вызовов чата и эмбеддингов.

**Возвращаемое значение**

* *(конструктор; неприменимо)*

**Подробно**

Этот конструктор инициализирует клиент `AI` тремя основными параметрами конфигурации, необходимыми для общения с удалённым сервисом:

* **Базовый URL** – все запросы отправляются относительно этого корня.
* **API-ключ** – обычно передаётся как HTTP-заголовок (например, `Authorization: Bearer <key>`).
* **Имя модели** – используется для выбора, какую модель должен запустить сервис (например, варианты для чата и эмбеддингов).

После создания экземпляр `AI` можно переиспользовать для множества запросов. Позже вы можете изменить модель через `setModel` или изменить другое поведение (системный промпт, JSON-режим, max tokens) без пересоздания клиента.

Если конфигурация неверна (например, неправильный URL или ключ), такие запросы, как `ask` и `createEmbedding`, завершатся ошибкой; это следует обнаруживать, проверяя возвращаемые `String` или `Embedding` объекты.

**Пример**

```cpp
using namespace attoboy;

String baseUrl("https://api.openai.com/v1");
String key("sk-123456");
String model("gpt-5-mini");

AI ai(baseUrl, key, model);
```

*Этот пример создаёт клиент `AI`, настроенный на общение с гипотетическим удалённым сервисом с указанной моделью.*

---

#### `AI(const AI &other)`

**Сигнатура**

```cpp
AI(const AI &other);
```

**Краткое описание**
Создаёт копию (разделяет базовую конфигурацию).

**Параметры**

* `other` – Существующий экземпляр `AI` для копирования.

**Возвращаемое значение**

* *(конструктор; неприменимо)*

**Подробно**

Конструктор копирования создаёт новый объект `AI`, который разделяет ту же внутреннюю конфигурацию и реализацию, что и `other`. Это **поверхностная копия**:

* Базовый URL, API-ключ, модель, системный промпт и настройки подсчёта токенов логически разделяются.
* Использование любого из экземпляров для отправки запросов или изменения конфигурации влияет на внутреннее состояние клиента.

Это удобно, когда вы хотите передать `AI` по значению в вспомогательный код, не беспокоясь о времени жизни или дорогих копиях.

**Пример**

```cpp
using namespace attoboy;

AI original(String("https://api.openai.com/v1"),
            String("sk-123456"),
            String("gpt-5-mini"));

AI copy(original);
```

*Этот пример создаёт второй дескриптор `AI`, который разделяет конфигурацию оригинала.*

---

#### `~AI()`

**Сигнатура**

```cpp
~AI();
```

**Краткое описание**
Уничтожает клиент AI и освобождает ресурсы.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* *(деструктор; неприменимо)*

**Подробно**

Когда объект `AI` уничтожается (например, выходит из области видимости), деструктор освобождает все ресурсы, связанные с клиентом:

* Внутренние дескрипторы и буферы.
* Сетевое и конфигурационное состояние, хранящееся в `AIImpl`.

Поскольку `AI` использует RAII, вам не нужно вручную закрывать сетевые соединения; внутренняя реализация заботится о чистке.

Если есть другие экземпляры `AI`, которые разделяют ту же реализацию (через копирование или присваивание), реализация остаётся живой до тех пор, пока не будет уничтожен последний дескриптор.

**Пример**

```cpp
using namespace attoboy;

{
  AI ai(String("https://api.openai.com/v1"),
        String("sk-123456"),
        String("gpt-5-mini"));
  // Используйте ai здесь
} // ai уничтожается; его ресурсы освобождаются
```

*Этот пример показывает экземпляр `AI` с автоматическим временем жизни внутри блока.*

---

#### `AI &operator=(const AI &other)`

**Сигнатура**

```cpp
AI &operator=(const AI &other);
```

**Краткое описание**
Присваивает другой клиент AI (разделяет базовую конфигурацию).

**Параметры**

* `other` – Существующий `AI`, из которого выполняется присваивание.

**Возвращаемое значение**

* Ссылка на `*this`, позволяющая цепочку присваиваний.

**Подробно**

Оператор присваивания заставляет левый операнд `AI` разделять ту же внутреннюю реализацию и конфигурацию, что и `other`. Любая предыдущая ассоциация `*this` освобождается, и ресурсы, которыми он владел уникально, могут быть освобождены.

После присваивания:

* Оба объекта `AI` ссылаются на тот же базовый URL, API-ключ, модель, системный промпт и состояние подсчёта токенов.
* Отправка запросов через любой дескриптор использует общую конфигурацию.

Самоприсваивание безопасно; реализация должна его обнаруживать и избегать ненужной работы.

**Пример**

```cpp
using namespace attoboy;

AI ai1(String("https://api.openai.com/v1"),
       String("sk-1"),
       String("gpt-5-mini"));

AI ai2(String("https://api.other.com/v1"),
       String("sk-2"),
       String("gpt-5-mini"));

ai2 = ai1;
// ai2 теперь разделяет конфигурацию ai1
```

*Этот пример переназначает экземпляр `AI`, чтобы он разделял конфигурацию другого.*

---

#### `AI &setModel(const String &model)`

**Сигнатура**

```cpp
AI &setModel(const String &model);
```

**Краткое описание**
Устанавливает имя модели. Возвращает этот AI для цепочек вызовов.

**Параметры**

* `model` – Новое имя модели для последующих запросов.

**Возвращаемое значение**

* Ссылка на `*this` (тот же объект `AI`), позволяющая удобное цепочное программирование.

**Подробно**

`setModel` обновляет модель по умолчанию для всех будущих операций, выполняемых с этим экземпляром `AI`, и для всех новых разговоров, созданных из него. Оно не влияет на операции, которые уже были отправлены.

Это полезно, когда:

* Вы хотите переключаться между лёгкими и тяжёлыми моделями (например, `"small-quick"` vs `"large-accurate"`).
* Хотите использовать одну модель для чата и другую для эмбеддингов через разные экземпляры `AI`.

Поскольку метод возвращает `*this`, вы можете писать цепочки вызовов:

```cpp
ai.setModel("gpt-5-mini").setMaxTokens(256);
```

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setModel("gpt-4.1-large");
```

*Этот пример переключает клиент AI на другую модель для будущих вызовов.*

---

#### `AI &setSystemPrompt(const String &prompt)`

**Сигнатура**

```cpp
AI &setSystemPrompt(const String &prompt);
```

**Краткое описание**
Устанавливает системный промпт (пустая строка для очистки). Возвращает этот AI для цепочек вызовов.

**Параметры**

* `prompt` – `String`, содержащий системный промпт, или пустая строка для его очистки.

**Возвращаемое значение**

* Ссылка на `*this`, позволяющая удобное цепочное программирование.

**Подробно**

Системный промпт предоставляет **глобальные инструкции**, которые влияют на поведение AI во всех запросах для данного экземпляра `AI`:

* Обычно он описывает роль ассистента, стиль и ограничения.
* Он комбинируется с каждым пользовательским промптом при вызовах `ask` и `Conversation::ask`.

Чтобы задать системный промпт, передайте `String`, содержащую инструкции.

Чтобы очистить системный промпт, передайте пустую строку. После вызова клиент вернётся к состоянию без системного промпта.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String sys(
  "You are a helpful assistant that answers concisely.\n"
  "Use simple language and avoid technical jargon."
);

ai.setSystemPrompt(sys);
```

*Этот пример настраивает системный промпт, формирующий ответы AI на все будущие запросы.*

---

#### `AI &setMaxTokens(int max = -1)`

**Сигнатура**

```cpp
AI &setMaxTokens(int max = -1);
```

**Краткое описание**
Устанавливает максимальное количество токенов в ответе (-1 для значения по умолчанию модели). Возвращает этот AI для цепочек вызовов.

**Параметры**

* `max` – Максимальное количество токенов, которое модель может сгенерировать в ответ.

  * `-1` использует значение по умолчанию модели (без явного переопределения).

**Возвращаемое значение**

* Ссылка на `*this`, позволяющая удобное цепочное программирование.

**Подробно**

`setMaxTokens` контролирует **максимальную длину** генерируемых ответов:

* Меньшие значения снижают затраты и задержку, но могут усекать более длинные ответы.
* Большие значения позволяют получать более детализованные ответы, но могут быть медленнее и дороже.

Когда ответ усечён из-за этого лимита, `getFinishReason()` обычно возвращает `"length"`. Когда модель завершает естественно, она возвращает `"stop"` или другую причину.

Эта настройка применяется к:

* `ask`
* `Conversation::ask`
* Возможно к запросам эмбеддингов (в зависимости от удалённого API), но в основном — к чат-запросам.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setMaxTokens(128);  // ограничить ответы 128 токенами
```

*Этот пример задаёт предел токенов, чтобы ответы были короткими и предсказуемыми.*

---

#### `AI &setJsonMode(bool isJsonMode = false)`

**Сигнатура**

```cpp
AI &setJsonMode(bool isJsonMode = false);
```

**Краткое описание**
Включает/отключает JSON-режим ответов. Возвращает этот AI для цепочек вызовов.

**Параметры**

* `isJsonMode` – `true` для включения JSON-режима; `false` для отключения.

**Возвращаемое значение**

* Ссылка на `*this`, позволяющая удобное цепочное программирование.

**Подробно**

При включённом JSON-режиме:

* Клиент `AI` настраивает запрос так, чтобы от модели ожидалась JSON-структура.
* Ваши промпты и системные промпты должны чётко инструктировать модель отвечать валидным JSON (например, одиночным JSON-объектом или массивом).

Этот режим особенно полезен для:

* Инструментов, которые парсят ответы программно с помощью attoboy `Map`/`List` после разбора JSON.
* Рабочих процессов, зависящих от структурированного вывода (например, метки классификации, конфигурационные объекты).

JSON-режим не выполняет автоматический разбор ответа; `ask` по-прежнему возвращает `String`. Вы сами отвечаете за разбор этой строки в структурированные данные по мере необходимости.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setJsonMode(true);
```

*Этот пример включает JSON-режим, чтобы последующие ответы ожидались в формате JSON.*

---

#### `String getModel() const`

**Сигнатура**

```cpp
String getModel() const;
```

**Краткое описание**
Возвращает текущее имя модели.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* `String`, содержащий имя модели, сконфигурированной для этого `AI`.

**Подробно**

`getModel()` позволяет узнать, какую модель клиент в данный момент использует. Это полезно для:

* Отладки (логирование конфигурации перед запросом).
* Создания инструментов, которые отображают или сообщают выбор модели.

Возвращённый `String` является неизменяемой копией внутреннего имени модели.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String currentModel = ai.getModel();  // "gpt-5-mini"
```

*Этот пример получает текущее имя модели для логирования или отображения.*

---

#### `String getSystemPrompt() const`

**Сигнатура**

```cpp
String getSystemPrompt() const;
```

**Краткое описание**
Возвращает системный промпт. Проверьте `isEmpty()`, чтобы узнать, установлен ли он.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* `String`, содержащий текущий системный промпт, или пустую строку, если он не установлен.

**Подробно**

`getSystemPrompt()` возвращает то, что было последний раз настроено через `setSystemPrompt`, или пустую строку, если системный промпт никогда не был установлен или был очищен.

Поскольку пустая строка также является допустимым промптом, используйте `isEmpty()` на возвращённом значении, чтобы отличить «системный промпт не установлен» от «явно установлен пустой».

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String sys = ai.getSystemPrompt();
if (sys.isEmpty()) {
  // Системный промпт не настроен
}
```

*Этот пример проверяет, настроен ли системный промпт.*

---

#### `String getBaseUrl() const`

**Сигнатура**

```cpp
String getBaseUrl() const;
```

**Краткое описание**
Возвращает базовый URL.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* `String`, содержащий базовый URL для этого клиента `AI`.

**Подробно**

`getBaseUrl()` предоставляет базовую конечную точку, которую использует этот экземпляр `AI`. Это полезно для:

* Отладки многосредовых развертываний (например, staging vs. production).
* Логирования конфигурации при проведении диагностики.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String url = ai.getBaseUrl();  // "https://api.openai.com/v1"
```

*Этот пример запрашивает базовый URL, чтобы подтвердить, к какому окружению подключается клиент.*

---

#### `String getAPIKey() const`

**Сигнатура**

```cpp
String getAPIKey() const;
```

**Краткое описание**
Возвращает API-ключ.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* `String`, содержащий API-ключ.

**Подробно**

`getAPIKey()` возвращает API-ключ, сохранённый в этом экземпляре `AI`. Это в первую очередь полезно для:

* Подтверждения того, что клиент настроен каким-то ключом.
* Передачи ключа в другие API, которым он может понадобиться.

Поскольку API-ключи чувствительны, избегайте их прямой записи в лог. Если необходимо логировать, рассмотрите возможность маскирования или усечения ключа перед выводом.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String key = ai.getAPIKey();
// Избегайте прямого вывода ключа в логах.
```

*Этот пример получает API-ключ с напоминанием обращаться с ним как с конфиденциальной информацией.*

---

#### `int getPromptTokensUsed() const`

**Сигнатура**

```cpp
int getPromptTokensUsed() const;
```

**Краткое описание**
Возвращает кумулятивное количество использованных промпт-токенов.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* Общее число промпт-токенов, потреблённых этим экземпляром `AI` с момента создания или последнего вызова `resetTokenTracking()`.

**Подробно**

Каждый раз, когда вы вызываете `ask` или используете `Conversation`, созданный из этого `AI`, промпт-токены учитываются и добавляются к этому общему значению. Промпт-токены обычно включают:

* Токены системного промпта.
* Историю разговора или сообщений.
* Новый входной промпт.

Используйте это значение для мониторинга того, сколько входных данных вы отправили модели с течением времени.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Hello!", -1);
int promptTokens = ai.getPromptTokensUsed();
```

*Этот пример получает накопленное количество промпт-токенов после выполнения запроса.*

---

#### `int getResponseTokensUsed() const`

**Сигнатура**

```cpp
int getResponseTokensUsed() const;
```

**Краткое описание**
Возвращает кумулятивное количество токенов ответа.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* Общее число токенов ответа, сгенерированных моделью для этого экземпляра `AI` с момента создания или последнего вызова `resetTokenTracking()`.

**Подробно**

Токены ответа измеряют объём **выходных данных**, сгенерированных моделью в ответ на ваши запросы. Это включает:

* Одноразовые ответы из `ask`.
* Ответы из `Conversation::ask`.

Это значение полезно для мониторинга и ограничения общего объёма выдачи модели.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Explain RAII briefly.", -1);
int responseTokens = ai.getResponseTokensUsed();
```

*Этот пример запрашивает общее количество токенов, сгенерированных моделью до текущего момента.*

---

#### `int getTotalTokensUsed() const`

**Сигнатура**

```cpp
int getTotalTokensUsed() const;
```

**Краткое описание**
Возвращает суммарное кумулятивное количество использованных токенов.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* Сумма промпт- и ответных токенов, потреблённых этим экземпляром `AI`.

**Подробно**

`getTotalTokensUsed()` — удобный шорткат для:

```cpp
getPromptTokensUsed() + getResponseTokensUsed()
```

Используйте это, когда вас интересует суммарное использование, например для приблизительных расчётов затрат.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("Give me a short poem.", -1);

int totalTokens = ai.getTotalTokensUsed();
```

*Этот пример получает суммарное использование токенов (вход + выход).*

---

#### `void resetTokenTracking()`

**Сигнатура**

```cpp
void resetTokenTracking();
```

**Краткое описание**
Сбрасывает все счётчики токенов в ноль.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* *(void; неприменимо)*

**Подробно**

Вызов `resetTokenTracking()` устанавливает:

* `getPromptTokensUsed()` → 0
* `getResponseTokensUsed()` → 0
* `getTotalTokensUsed()` → 0

Это полезно, когда вы хотите:

* Начать новый отчётный период (например, для каждого выполнения команды или сессии пользователя).
* Отбросить предыдущие значения перед измерением использования для одной операции.

Сброс влияет только на будущие вызовы; предыдущие данные после сброса восстановить нельзя.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.ask("First request.", -1);
ai.resetTokenTracking();
ai.ask("Second request.", -1);

int tokensForSecond = ai.getTotalTokensUsed();
```

*Этот пример сбрасывает отслеживание токенов после первого запроса и измеряет использование только для второго.*

---

#### `String getFinishReason() const`

**Сигнатура**

```cpp
String getFinishReason() const;
```

**Краткое описание**
Возвращает причину завершения последнего вызова (например, "stop", "length").

**Параметры**

* *(нет)*

**Возвращаемое значение**

* `String`, описывающий, почему последний вызов модели перестал генерировать вывод.

**Подробно**

`getFinishReason()` даёт представление о том, как завершился последний `ask` или `Conversation::ask`. Типичные значения включают:

* `"stop"` – модель завершила естественно (например, дошла до токена конца последовательности или выполнила ограничения).
* `"length"` – модель остановлена, потому что достигнут лимит максимального количества токенов (`setMaxTokens` или лимит сервиса).
* Другие значения, специфичные для сервиса (например, `"content_filter"`, `"tool_calls"` и т. п.).

Если запросов ещё не было, причина завершения может быть пустой строкой или иметь значение по умолчанию в зависимости от реализации.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

ai.setMaxTokens(16);
String answer = ai.ask("Explain RAII in detail.", -1);

String reason = ai.getFinishReason();
// например, "length", если объяснение было усечено
```

*Этот пример проверяет, был ли ответ усечён из-за лимита токенов.*

---

#### `String ask(const String &prompt, int timeout = -1)`

**Сигнатура**

```cpp
String ask(const String &prompt, int timeout = -1);
```

**Краткое описание**
Отправляет одиночный промпт и возвращает ответ. В случае ошибки проверьте `isEmpty()`.

**Параметры**

* `prompt` – Входной текст или инструкция для модели.
* `timeout` – Таймаут в миллисекундах.

  * `-1` означает «использовать значение по умолчанию модели или клиента» (часто трактуется как бесконечный таймаут или настроенное значение по умолчанию).

**Возвращаемое значение**

* `String`, содержащий текст ответа модели при успешном выполнении.
* Пустой `String`, если произошла ошибка (например, сетевая ошибка, неверная конфигурация или ошибка сервиса).

**Подробно**

`ask` — самый простой способ взаимодействия с AI:

* Он формирует одношаговый запрос, используя текущую конфигурацию:

  * Base URL, API key, model.
  * Системный промпт (если есть).
  * Max tokens и JSON-режим.
* Ожидает ответа (или пока не истечёт `timeout`).
* Возвращает ответ модели как `String`.

Типичные шаблоны для обработки ошибок:

* Если `ask` возвращает пустую строку, следует считать это ошибкой и обработать соответствующим образом (например, через логирование или повторную попытку).
* Используйте `getFinishReason()` чтобы узнать, как завершился запрос.
* Используйте методы отслеживания токенов для измерения использования.

Поведение таймаута:

* Положительное значение `timeout` ограничивает время ожидания ответа.
* `-1` оставляет поведение с таймаутом на усмотрение реализации и удалённого сервиса.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

String response = ai.ask("Give me a one-line definition of RAII.", 5000);

if (!response.isEmpty()) {
  // Используйте ответ, например, выведите или залогируйте
}
```

*Этот пример отправляет промпт с таймаутом 5 секунд и проверяет пустую строку для обнаружения ошибок.*

---

#### `Embedding createEmbedding(const String &str, int dimensions = -1, int timeout = -1)`

**Сигнатура**

```cpp
Embedding createEmbedding(const String &str, int dimensions = -1,
                          int timeout = -1);
```

**Краткое описание**
Создаёт вектор-эмбеддинг. В случае ошибки проверьте `getDimensions() == 0`.

**Параметры**

* `str` – Текст для встраивания (вход, семантический смысл которого вы хотите захватить).
* `dimensions` – Желаемое число измерений для эмбеддинга:

  * `-1` позволяет модели выбрать стандартную размерность по умолчанию.
  * Положительные значения запрашивают конкретный размер, если сервис поддерживает такую опцию.
* `timeout` – Таймаут в миллисекундах для запроса (`-1` для поведения по умолчанию).

**Возвращаемое значение**

* Объект `Embedding`, представляющий вектор.
* В случае ошибки — `Embedding`, у которого `getDimensions() == 0`.

**Подробно**

Эмбеддинги преобразуют текст в числовые векторы, отражающие семантическое сходство:

* Похожие тексты дают эмбеддинги с более высоким косинусным сходством (близким к `1.0`).
* Непохожие тексты дают эмбеддинги с низким или отрицательным сходством.

После вызова `createEmbedding`:

* Проверьте `embedding.getDimensions()`:

  * `> 0` – успешно.
  * `0` – ошибка (неверная конфигурация, сетевая проблема или ошибка сервиса).
* Используйте `Embedding::compare` для вычисления сходства между двумя эмбеддингами.

Параметр `dimensions` позволяет контролировать размер вектора, когда сервис поддерживает несколько размеров. Меньшие векторы быстрее и занимают меньше памяти; большие векторы могут быть точнее.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("text-embedding-model"));

Embedding e1 = ai.createEmbedding("Hello world", -1, 5000);
Embedding e2 = ai.createEmbedding("Hi there", -1, 5000);

if (e1.getDimensions() > 0 && e2.getDimensions() > 0) {
  float similarity = e1.compare(e2);
  // similarity близко к 1.0 означает семантическое сходство текстов
}
```

*Этот пример создаёт эмбеддинги для двух коротких фраз и сравнивает их с помощью косинусного сходства.*

---

#### `Conversation createConversation()`

**Сигнатура**

```cpp
Conversation createConversation();
```

**Краткое описание**
Создаёт новый многоходовой разговор.

**Параметры**

* *(нет)*

**Возвращаемое значение**

* Объект `Conversation`, связанный с этим экземпляром `AI`.

**Подробно**

`createConversation` конструирует `Conversation`, который:

* Использует тот же базовый URL, API-ключ, модель, системный промпт, max tokens и JSON-режим, что и экземпляр `AI`, создавший его.
* Поддерживает собственную историю сообщений:

  * Каждый вызов `Conversation::ask` добавляет сообщения пользователя и ассистента в разговор.
  * Модель видит всю историю при генерации новых ответов.

Используйте `Conversation`, когда вам нужны:

* Состояниесберегающие взаимодействия, где каждый ответ зависит от предыдущих вопросов.
* Чат-подобный интерфейс внутри вашего приложения.

Вы можете создавать несколько разговоров из одного экземпляра `AI`; каждый из них имеет свою независимую историю.

**Пример**

```cpp
using namespace attoboy;

AI ai(String("https://api.openai.com/v1"),
      String("sk-123456"),
      String("gpt-5-mini"));

Conversation conv = ai.createConversation();

String reply1 = conv.ask("Hello, who are you?", -1);
String reply2 = conv.ask("Can you remind me what I just asked?", -1);
```

*Этот пример создаёт разговор и выполняет два хода диалога, где второй вопрос зависит от первого.*